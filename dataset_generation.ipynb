{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fac0c21",
   "metadata": {},
   "source": [
    "# Spotify Songs Network - Dataset Generation\n",
    "* In this notebook we will create the dataset that we will use to create a Network about Spotify Songs, based on user's Playlists.\n",
    "* Specifically, we want to create a Network with the following characteristics\n",
    "  * **Nodes**: Songs\n",
    "  * **Edges**: will be created between songs if the songs are found in the same playlist.\n",
    "* In this notebook, we will create our dataset, and to do that we will obtain data from:\n",
    "  1. [Spotify Playlists](https://www.kaggle.com/andrewmvd/spotify-playlists) Dataset from [Kaggle](https://www.kaggle.com/).\n",
    "    * Pichl, Martin; Zangerle, Eva; Specht, GÃ¼nther: \"Towards a Context-Aware Music Recommendation Approach: What is Hidden in the Playlist Name?\" in 15th IEEE International Conference on Data Mining Workshops (ICDM 2015), pp. 1360-1365, IEEE, Atlantic City, 2015.\n",
    "    * **License**: CC BY 4.0\n",
    "  2. [Spotify Web API](https://developer.spotify.com/documentation/web-api/)\n",
    "  3. [Chosic Music Genre Finder](https://www.chosic.com/music-genre-finder/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c034c4e",
   "metadata": {},
   "source": [
    "## Spotify for Developers Credentials\n",
    "* In case a user of this notebook wants to execute the cells that create a connection with the [Spotify's Web API](https://developer.spotify.com/documentation/web-api/) it is necessary to create an application at http://developer.spotify.com.\n",
    "* In that way the user will get a client ID and a client secret.\n",
    "* Then, they have to create a file `spotify_config.py` with the following contents:\n",
    "\n",
    "  ```\n",
    "  config = {\n",
    "      'client_id' : 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX',\n",
    "      'client_secret' :'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "  }\n",
    "  ```\n",
    "  where instead of Xs there are the client ID and client secret of the user.\n",
    "* This file will be placed in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a344fb",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "* To begin with, we will import the packages, that we will use in the following segments of the project:\n",
    "    * [pandas](https://pandas.pydata.org/)\n",
    "    * [Spotipy](https://spotipy.readthedocs.io/en/2.19.0/)\n",
    "    * [webdriver-manager](https://pypi.org/project/webdriver-manager/)\n",
    "    * [Selenium](https://selenium-python.readthedocs.io/)\n",
    "    * [Beautiful Soup](https://beautiful-soup-4.readthedocs.io/en/latest/)\n",
    "* Note that the prementioned packages **must be locally installed too** in order to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b44835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "\n",
    "import random\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037102b7",
   "metadata": {},
   "source": [
    "## Kaggle Dataset\n",
    "* As mentioned above, we will get the basic data from [Spotify Playlists](https://www.kaggle.com/andrewmvd/spotify-playlists) Dataset from [Kaggle](https://www.kaggle.com/).\n",
    "* After downloading it, we have to create a folder <code>data</code> and put it into it, under the name <code>spotify_dataset.csv.zip</code>.\n",
    "* So, let's read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ef239c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>\"artistname\"</th>\n",
       "      <th>\"trackname\"</th>\n",
       "      <th>\"playlistname\"</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9cc0cfd4d7d7885102480dd99e7a90d6</td>\n",
       "      <td>Elvis Costello</td>\n",
       "      <td>(The Angels Wanna Wear My) Red Shoes</td>\n",
       "      <td>HARD ROCK 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9cc0cfd4d7d7885102480dd99e7a90d6</td>\n",
       "      <td>Elvis Costello &amp; The Attractions</td>\n",
       "      <td>(What's So Funny 'Bout) Peace, Love And Unders...</td>\n",
       "      <td>HARD ROCK 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9cc0cfd4d7d7885102480dd99e7a90d6</td>\n",
       "      <td>Tiffany Page</td>\n",
       "      <td>7 Years Too Late</td>\n",
       "      <td>HARD ROCK 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9cc0cfd4d7d7885102480dd99e7a90d6</td>\n",
       "      <td>Elvis Costello &amp; The Attractions</td>\n",
       "      <td>Accidents Will Happen</td>\n",
       "      <td>HARD ROCK 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9cc0cfd4d7d7885102480dd99e7a90d6</td>\n",
       "      <td>Elvis Costello</td>\n",
       "      <td>Alison</td>\n",
       "      <td>HARD ROCK 2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            user_id                      \"artistname\"  \\\n",
       "0  9cc0cfd4d7d7885102480dd99e7a90d6                    Elvis Costello   \n",
       "1  9cc0cfd4d7d7885102480dd99e7a90d6  Elvis Costello & The Attractions   \n",
       "2  9cc0cfd4d7d7885102480dd99e7a90d6                      Tiffany Page   \n",
       "3  9cc0cfd4d7d7885102480dd99e7a90d6  Elvis Costello & The Attractions   \n",
       "4  9cc0cfd4d7d7885102480dd99e7a90d6                    Elvis Costello   \n",
       "\n",
       "                                         \"trackname\"  \"playlistname\"  \n",
       "0               (The Angels Wanna Wear My) Red Shoes  HARD ROCK 2010  \n",
       "1  (What's So Funny 'Bout) Peace, Love And Unders...  HARD ROCK 2010  \n",
       "2                                   7 Years Too Late  HARD ROCK 2010  \n",
       "3                              Accidents Will Happen  HARD ROCK 2010  \n",
       "4                                             Alison  HARD ROCK 2010  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/spotify_dataset.csv.zip', on_bad_lines='skip')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014412d",
   "metadata": {},
   "source": [
    "* Next, we will rename the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dd4e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={' \"artistname\"' : 'Artist', ' \"trackname\"': 'Track_Name', ' \"playlistname\"': 'Playlist_Name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f073c763",
   "metadata": {},
   "source": [
    "* Because our dataset contains too many songs we will **keep** only those that are included in more than 500 playlists.\n",
    "* We will do that because if we have to many nodes in our Network, it will not be easily **interpretable**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8716c7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/44888858/how-to-drop-unique-rows-in-a-pandas-dataframe\n",
    "df = df[df.groupby(['Track_Name', 'Artist'])['Track_Name'].transform('size') > 500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390ea0a0",
   "metadata": {},
   "source": [
    "* Also, we will combine the columns <code>user_id</code> and <code>Playlist_Name</code> into one, in order to be our data more concentrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c99eadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Playlist'] = df.apply(lambda row: str(row['Playlist_Name']) + \" by \" + row['user_id'], axis=1)\n",
    "df.drop(columns=['user_id', 'Playlist_Name'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10eee94",
   "metadata": {},
   "source": [
    "## Edges Creation\n",
    "* Next, we will create our edges, that will be **weighted**.\n",
    "* Each edge will have a *Source*, a *Target* and a *Weight*.\n",
    "* The *Weight* will be the number of Playlists that the two songs are included together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c202631",
   "metadata": {},
   "source": [
    "* Before doing that, some songs contain in their names characters that make them not searchable using the API, so we will slightly modify their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b95e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_name_mapping = {\n",
    "    \"Baba O'Riley - Original Album Version\" : \"Baba O'Riley\",\n",
    "    'Jerk It Out - Original Mix' : 'Jerk It Out',\n",
    "    'Jump - Remastered Version' : 'Jump',\n",
    "    \"Don't You Worry Child (Radio Edit) [feat. John Martin]\" : \"Don't You Worry Child Radio Edit\",\n",
    "    'Save the World - Radio Mix' : 'Save the World',\n",
    "    'Wildfire (feat. Little Dragon)' : 'Wildfire',\n",
    "    'Blister In The Sun (Remastered Album Version)' : 'Blister In The Sun',\n",
    "    'Hey Ya! - Radio Mix / Club Mix' : 'Hey Ya!',\n",
    "    'How Soon Is Now? (2008 Remastered Version)' : 'How Soon Is Now?',\n",
    "    'Intergalactic - 2009 Digital Remaster' : 'Intergalactic',\n",
    "    'This Charming Man (2008 Remastered Version)' : 'This Charming Man',\n",
    "    'Suit & Tie featuring JAY Z' : 'Suit & Tie',\n",
    "    'A-Punk (Album)' : 'A-Punk',\n",
    "    'Heroes - 1999 Remastered Version' : 'Heroes',\n",
    "    'Sexy Bitch (feat. Akon) - Featuring Akon;explicit' : 'Sexy Bitch',\n",
    "    'Wannabe - Radio Edit' : 'Wannabe'    \n",
    "}\n",
    "\n",
    "df['Track_Name'] = df['Track_Name'].map(lambda x: track_name_mapping.get(x, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d233c96d",
   "metadata": {},
   "source": [
    "* Now, we are ready to create the nodes of our Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463762b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = df[['Track_Name', 'Artist']].copy().drop_duplicates()\n",
    "nodes.reset_index(inplace=True, drop=True)\n",
    "nodes['Id'] = nodes.index\n",
    "nodes.rename(columns={'Track_Name': 'Label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607e46c8",
   "metadata": {},
   "source": [
    "* Then, we will create a new column into the datast, that will contain the *Node ID* for each track.\n",
    "* To do that, we will use a mapping, with keys the name of the track and the artist and values the id of the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a1440f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tracks_artist = list(zip(nodes['Label'], nodes['Artist']))\n",
    "nodes_id_mapping = dict(zip(list_tracks_artist, nodes['Id']))\n",
    "\n",
    "df['Track_Id'] = df.apply(lambda row: nodes_id_mapping[(row['Track_Name'], row['Artist'])], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e900e1c",
   "metadata": {},
   "source": [
    "* The next thing we will do is to count the number of playlists that each pair of tracks are included together.\n",
    "* To do that, first we will **group** our dataframe using the <code>Playlist</code> column.\n",
    "* And then, we will use [itertools](https://docs.python.org/3/library/itertools.html#itertools) to get all the possible pairs within each playlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02755ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df.groupby('Playlist')\n",
    "\n",
    "pair_counts = defaultdict(int)\n",
    "for name, group in df_grouped:\n",
    "    try:\n",
    "        pairs = list(combinations(group['Track_Id'], 2))\n",
    "        for pair in pairs:\n",
    "            pair_sorted = tuple(sorted(list(pair)))\n",
    "            pair_counts[pair_sorted] += 1\n",
    "    except MemoryError:\n",
    "        print('Group {} is too big, it contains {} rows.'.format(name, len(group)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9ecaae",
   "metadata": {},
   "source": [
    "* We are now ready to extract our edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0324f86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def write_headers(writer):\n",
    "    headers = ['Source', 'Target', 'Weight']\n",
    "    writer.writerow(headers)\n",
    "\n",
    "def write_edges(writer, edges_weights_dict):\n",
    "    for edge in edges_weights_dict.keys():\n",
    "        edge_row = [edge[0], edge[1], edges_weights_dict[edge]]\n",
    "        writer.writerow(edge_row)\n",
    "\n",
    "f = open('network_data/edges.csv', 'w', newline='')\n",
    "writer = csv.writer(f)\n",
    "\n",
    "write_headers(writer)\n",
    "write_edges(writer, pair_counts)\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
